# Activation_Functions_and_Naive_NeuralNetwork
This is a naive implementation of all important(significant :innocent:) activation functions and also a Neural Network from scratch by assiging manual weights to each of the neurons (Hidden)

## Activation Functions Experimented on 

### 
- Linear Function
- Sigmoid Function
 - TanH Function
 - ReLU Function
 - Leaky ReLU Function
 - Parametric Function
 - Softplus Function
 - Exponential Linear Unit Function
 - Binary Step Function
 - ArcTan Function
 - Gaussian Function
 - Swish Function
 - Bipolar Function
