# Activation_Functions_and_Naive_NeuralNetwork
This is a naive implementation of all important(significant :innocent:) activation functions and also a Neural Network from scratch by assiging manual weights to each of the neurons (Hidden)

## Analysis on various Activation Functions 

### 
- **Linear Function :-** Linear activation functions are better in giving a wide range of activations and a line of a positive slope may increase the firing rate as the input rate increases.

- **Sigmoid :-** 
 
- **TanH :-** 

- **Recitified Linear Unit(ReLU):-** 

- **Leaky ReLU :-** 

- **Parametric :-** 

- **Softplus :-** 

- **Exponential Linear Unit :-** 

- **Binary Step :-** 

- **ArcTan :-** 

- **Gaussian :-** 

- **Swish :-** 

- **Bipolar :-** 
