# Activation_Functions_and_Naive_NeuralNetwork
This is a naive implementation of all important(significant :innocent:) activation functions and also a Neural Network from scratch by assiging manual weights to each of the neurons (Hidden)

## Analysis on various Activation Functions 

### 
- **Linear Function :-** Linear activation functions are better in giving a wide range of activations and a line of a positive slope may increase the firing rate as the input rate increases.

- **Sigmoid :-** When using a sigmoid activation function , a small change in the input only causes a small change in the output. 
  The pros are:-  
  #### 
  - function is differentiable.
  - monotonic
  - has a smooth gradient
  - has probablistic range of 0 to 1 (normalization)
  -  For X above 2 or below -2, tends to bring the Y value (the prediction) to the edge of the curve, very close to 1 or 0. This enables clear predictions.
 
 The cons are:- 
 
 #### 
  - The derivative values in the regions where y values react very little to the changes in x are very small and converge to 0.This is called the vanishing gradient and the learning is minimal. 
  - Outputs not zero centered.
  - Computationally more expensive
 
- **TanH :-** :- 

- **Recitified Linear Unit(ReLU):-** 

- **Leaky ReLU :-** 

- **Parametric :-** 

- **Softplus :-** 

- **Exponential Linear Unit :-** 

- **Binary Step :-** 

- **ArcTan :-** 

- **Gaussian :-** 

- **Swish :-** 

- **Bipolar :-** 
